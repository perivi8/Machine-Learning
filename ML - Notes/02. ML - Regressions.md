<<---------------------------------ML - Regressions ---------------------------------------------->>

```python

regression 

-> Simple Linear   ---{Ridge & lasso}    --20,29
-> Multiple Linear   ---{Ridge & lasso}  --21,29
-> Polynomial Regression                 --22,23
-> Support Vector Regression **          --24,25
-> Regression Tree / Descension tree **  --26
-> Random Forest regression **           --27


## For 2 columns Data no need to split the data..

-> Simple Linear Regression


from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x , y)

Y= reg.predict(x)
Y

plt.scatter(x , y , color = 'g')
plt.plot(x,Y , 'r')


# If we split the data means


from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x_train , y_train)

Y= reg.predict(x_train)
Y

plt.scatter(x_train , y_train , color = 'g')
plt.plot(x_train,Y , 'r')


Y= reg.predict(x_test)
Y

plt.scatter(x_test , y_test , color = 'g')
plt.plot(x_test,Y , 'r')

<<-------------------------------------------------------------------------------------------------->>

-> Multiple Linear regression



from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)


from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x_train , y_train)

Y= reg.predict(x_train)
Y

plt.scatter(y_train , Y , color ='r')
plt.plot([y_train.min(),y_train.max()],[y_train.min(),y_train.max()] , 'k--')


Y= reg.predict(x_test)
Y

plt.scatter(y_test , Y , color ='r')
plt.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()] , 'k--')


=============================

* Ridge & lasso

from sklearn.linear_model import Ridge, Lasso

rd = Ridge()
rd.fit(X_train,y_train)
rd.score(X_test, y_test)

=============================

ls = Lasso()
ls.fit(X_train,y_train)
ls.score(X_test, y_test)


ls2 = Lasso(alpha=2)
ls2.fit(X_train,y_train)
ls2.score(X_test, y_test)


ls2 = Lasso(alpha=3)
ls2.fit(X_train,y_train)
ls2.score(X_test, y_test)


<<-------------------------------------------------------------------------------------------------->>

-> Polynomial Regression  




* For 2 Colums Datasets

# Step-1
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(x , y)  

Z = reg.predict(x)  

plt.scatter(x,y)
plt.plot(x,Z , color = 'r') 


# Step-2
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree= 6)           
x_poly = poly.fit_transform(x)  
poly.fit(x_poly,y)                             


# Step-3
from sklearn.linear_model import LinearRegression
reg_2 = LinearRegression()
reg_2.fit(x_poly,y)  

Z1 = reg_2.predict(x_poly)

plt.scatter(x,y)
plt.plot(x,Z1, color = 'r') 


=============================

* For more than 2 Columns

# Step-1
from  sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(x_train , y_train)

Z = lr.predict(x_train)

plt.scatter(y_train , Z)
plt.plot([y_train.min() , y_train.max()],[min(y_train) , max(y_train)] , 'k--')


# Step-2
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

x_train_sc = sc.fit_transform(x_train)
x_test_sc = sc.transform(x_test)


# Step-3
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)

x_train_poly = poly.fit_transform(x_train_sc)
x_test_poly = poly.transform(x_test_sc)

poly.fit(x_train_poly , y_train)     


# Step-4
from sklearn.linear_model import LinearRegression
lr1 = LinearRegression()
lr1.fit(x_train_poly,y_train)    

Z2 = lr1.predict(x_train_poly)


<<-------------------------------------------------------------------------------------------------->>

-> Support Vector Regression **  




* For we have 2 Columns

# No need the Test data

# Step-1
from sklearn.preprocessing import StandardScaler   # Compulsary for both X & Y
sc = StandardScaler()

X = sc.fit_transform(x)          
Y = sc.fit_transform(y)

# Step-2
from sklearn.svm import SVR
sv_regression1 = SVR(kernel='rbf')
sv_regression1.fit(X , Y)

z1 = sv_regression1.predict(X)

plt.scatter(X , Y , color = 'r')
plt.plot(X, z1)


=============================

* For More than 2 Columns

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()


x_train_sc = sc.fit_transform(x_train)
x_test_sc = sc.transform(x_test)


from sklearn.svm import SVR
reg = SVR(kernel='rbf' ,  C=100, gamma=0.1, epsilon=0.1)
reg.fit(x_train_sc , y_train)

pred = reg.predict(x_test_sc)



<<-------------------------------------------------------------------------------------------------->>

-> Regression Tree / Descension tree **




# Step-1
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

# Step-2
from sklearn.tree import DecisionTreeRegressor
dt = DecisionTreeRegressor()
dt.fit(x_train , y_train)


dt.fit(x_train , y_train)


<<-------------------------------------------------------------------------------------------------->>

-> Random Forest regression ** 




from sklearn.preprocessing import StandardScaler
ss = StandardScaler()


x_train = ss.fit_transform(x_train)
x_test= ss.transform(x_test)


from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(x_train , y_train)

pred = rf.predict(x_test)





<<-------------------------------------------------------------------------------------------------->>


## This below steps will aplicable for all type of regressions


# First check without scalling (y , y_train , y_test) , if we got less accuarcy means then try this...
y_train= ss .fit_transform(y_train) 
y_test = ss.transform(y_test)


z = regression.predict([[10]])
z = ss.inverse_transform([z])   # This will use only if we make scaling for - [y , y_train , y_test]
z




from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

from sklearn.ensemble import RandomForestRegressor

